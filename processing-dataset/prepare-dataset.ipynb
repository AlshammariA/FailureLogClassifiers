{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge PIT mutation to test-xml-file (Flaky Failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import filecmp\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "from itertools import count, groupby\n",
    "import ast\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from lxml import etree\n",
    "import base64\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXMLFile(filePath):\n",
    "    parser = etree.XMLParser(strip_cdata=False,recover=True)\n",
    "    with open(filePath, \"rb\") as source:\n",
    "        tree = etree.parse(source, parser=parser)\n",
    "    xmlroot = tree.getroot()\n",
    "    return xmlroot\n",
    "\n",
    "def writeXML(xml,output):\n",
    "    outputXML = ET.ElementTree(xml)\n",
    "    outputXML.write(output+\".xml\")\n",
    "\n",
    "def save_dict_to_json(data_dict, file_name):\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=2)\n",
    "\n",
    "def cleanTestXML(testXML):\n",
    "    # This is called to remove the current mutants before merging ( avoid duplicate)\n",
    "    for child in testXML.findall(\".//mutant\"):\n",
    "        child.getparent().remove(child)\n",
    "    testXML.findall(\"./mutants\")[0].attrib.clear()\n",
    "    return testXML\n",
    "\n",
    "def getFilesByEndsWith(fileDir,ends):\n",
    "    filesList = []\n",
    "    for path, subdirs, files in os.walk(fileDir):\n",
    "        for file in files:\n",
    "            if (file.endswith(ends)):\n",
    "                filesList.append(os.path.join(path,file))\n",
    "    return filesList\n",
    "\n",
    "def getFilesByStartsWith(fileDir,starts):\n",
    "    filesList = []\n",
    "    for path, subdirs, files in os.walk(fileDir):\n",
    "        for file in files:\n",
    "            if (file.startswith(starts)):\n",
    "                filesList.append(os.path.join(path,file))\n",
    "    return filesList\n",
    "    \n",
    "def removeTestDirTag(testXML):\n",
    "    for child in testXML.iter('test_dir'):\n",
    "        child.getparent().remove(child)\n",
    "    return testXML\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_full_exception_and_stacktrace(message):\n",
    "    full_message = \"\"\n",
    "    stacktrace = \"\"\n",
    "    lines = message.split((\"\\n\"))\n",
    "    for l in range (0,len(lines)):\n",
    "        if lines[l].startswith(\"\\tat \") or lines[l].startswith(\"at \"):\n",
    "            if lines[l].strip():\n",
    "                stacktrace = stacktrace + lines[l] + \"\\n\"\n",
    "        else:\n",
    "            if lines[l].strip():\n",
    "                full_message = full_message + lines[l] + \" \"\n",
    "    return full_message.lstrip().rstrip(),stacktrace\n",
    "\n",
    "def remove_control_characters(s):\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "def remove_cdata_words(line):\n",
    "    new_line = re.sub(r'(\\[CDATA\\[(\\w*|\\w*)\\]\\])', '', str(line))\n",
    "    new_line = re.sub(r'(\\/\\/(\\s?)<!\\[CDATA\\[)', '', str(new_line))\n",
    "    #new_line = re.sub(r'(]]\\>(?!<\\/killingException>))', '))>', str(new_line))\n",
    "    return new_line\n",
    "\n",
    "\n",
    "def getExceptionWithMessageAndStacktraces(exception):\n",
    "    lines_as_arr = []\n",
    "    updated_exceptions = base64.b64decode(exception)\n",
    "    for line in updated_exceptions.splitlines():\n",
    "        updated_line = remove_cdata_words(line.decode('utf-8'))\n",
    "        updated_line = remove_control_characters(updated_line)\n",
    "        lines_as_arr.append(updated_line+\"\\n\")\n",
    "    full_exception,stackTrace = get_full_exception_and_stacktrace('\\t'.join(lines_as_arr))\n",
    "\n",
    "    return full_exception,stackTrace\n",
    "\n",
    "def appendMutant(xmlFile,status,totalKills,perException,mutant_name,mutantId,full_exception,stackTrace):\n",
    "    mutant = etree.SubElement(xmlFile.find('mutants'), \"mutant\")\n",
    "    etree.SubElement(mutant, \"mutant_name\", {'mutant_id': str(mutantId),'source': 'PIT20runs','status': str(status), 'numberOfKills': str(totalKills),\n",
    "                                            'killedByThisException': str(perException)}).text = mutant_name\n",
    "    etree.SubElement(mutant, \"mutant_exception\").text = re.sub('\\t','',full_exception)\n",
    "    etree.SubElement(mutant, \"mutant_stackTrace\").text = \"\\n\"+stackTrace\n",
    "    return xmlFile\n",
    "\n",
    "def mutationStat(xmlFile,mutant,mutantsDetails):\n",
    "    mutantXML = readXMLFile(mutant)\n",
    "    xmlFile.find('mutants').attrib['Total'] = str(len(mutantXML.findall('./mutation')))\n",
    "    xmlFile.find('mutants').attrib['Killed'] = str(len([k for k in mutantsDetails.keys() if k.endswith('KILLED')]))\n",
    "    xmlFile.find('mutants').attrib['Flaky'] = str(len([k for k in mutantsDetails.keys() if k.endswith('FLAKY')]))\n",
    "    return xmlFile\n",
    "\n",
    "def collectKilledMutants(pits,indx):\n",
    "    killedMutants = {}\n",
    "    xmlFilesPerTests = []\n",
    "    for pit in pits:\n",
    "        xmlFilesPerTests.append(readXMLFile(pit))\n",
    "    \n",
    "    for ind in indx:\n",
    "        status = [file[ind].attrib['status'] for file in xmlFilesPerTests]\n",
    "        \n",
    "        key = str(ind)\n",
    "        if (all(k == \"KILLED\" for k in status)):\n",
    "            key = key + '|KILLED'\n",
    "        elif('KILLED' in status and 'SURVIVED' in status):\n",
    "            key = key + '|FLAKY'\n",
    "        elif('FLAKY' in status):\n",
    "            key = key + '|FLAKY'\n",
    "        else:\n",
    "            key = 'skip'\n",
    "        if (key != 'skip'):\n",
    "            exceptions = []\n",
    "            for mutant in xmlFilesPerTests:\n",
    "                exceptions.extend([e.text for e in mutant[ind].iter('killingException')])\n",
    "            \n",
    "            killedMutants[key] = exceptions\n",
    "\n",
    "    return killedMutants\n",
    "\n",
    "\n",
    "def mergedMutants(test,killedMutants,test_name):\n",
    "    for k,v in killedMutants.items():\n",
    "        for e in list(set(v)):\n",
    "            exceptionWithMessage,stacttraces = getExceptionWithMessageAndStacktraces(e)\n",
    "            test = appendMutant(test,k.split('|')[1],len(v),v.count(e),test_name,k.split('|')[0],exceptionWithMessage,stacttraces)\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summurize xml functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSummaryXMLPlainFile():\n",
    "    # create plain xml file first\n",
    "    root = ET.Element(\"root\")\n",
    "    tests_xml = ET.SubElement(root, \"tests\")\n",
    "    mutants_xml = ET.SubElement(root, \"mutants\")\n",
    "    return root , tests_xml, mutants_xml\n",
    "\n",
    "def add_xml_block(tests_xml,block,test_name,freq,node_id,status,source,exception_type,target_lines,project_name):\n",
    "    if(block == \"test\"):        \n",
    "        test = ET.SubElement(tests_xml, \"test\")\n",
    "    else:\n",
    "        test = ET.SubElement(tests_xml, \"mutant\")\n",
    "    ET.SubElement(test, block+\"_name\", {'source': source,'id': str(node_id),'status': status, 'frequency': str(freq), 'project': str(project_name)}).text = test_name    \n",
    "    \n",
    "    test_exception = ET.SubElement(test, block+\"_exception\")    \n",
    "    test_exception.text = exception_type\n",
    "    \n",
    "    test_stacktracke = ET.SubElement(test, block+\"_stackTrace\")\n",
    "    for line in target_lines:\n",
    "        ET.SubElement(test_stacktracke, \"line\").text = line\n",
    "            \n",
    "    return tests_xml\n",
    "\n",
    "def get_main_exception_type(failure):\n",
    "\n",
    "    if (\":\" in failure):\n",
    "        exception_type = failure.split(':', 1)[0]\n",
    "        if (\".\" in exception_type):\n",
    "            return exception_type.rsplit('.', 1)[1]\n",
    "        else:\n",
    "            return exception_type\n",
    "    else:\n",
    "        if (\".\" in failure):\n",
    "            return failure.rsplit('.', 1)[1]\n",
    "        else:\n",
    "            return failure\n",
    "\n",
    "#%%\n",
    "def get_stacktrace(full_stackTrace,test_name):\n",
    "    if (\"Caused by:\" in full_stackTrace):\n",
    "        full_stackTrace = full_stackTrace.split(\"Caused by:\")[0]    \n",
    "    stackTrace_lines = full_stackTrace.split(\"at \")\n",
    "    updated_stackTrace_lines =[]\n",
    "    for line in stackTrace_lines:\n",
    "        if (len(line)>0):\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            line = line.replace(\"\\t\",\"\")     \n",
    "            updated_stackTrace_lines.append(line.lstrip().rstrip())\n",
    "    updated_stackTrace_lines = list(filter(None, updated_stackTrace_lines))\n",
    "    if (len([l for l in updated_stackTrace_lines if l.startswith(test_name+\"(\")])>0):\n",
    "        split_line_by = \"test_name\"\n",
    "        lines = [i for i in updated_stackTrace_lines if i.startswith(test_name+\"(\")]\n",
    "        stoped_index = updated_stackTrace_lines.index(lines[0])\n",
    "        target_lines = updated_stackTrace_lines[:stoped_index+1]\n",
    "        return target_lines,split_line_by,len(updated_stackTrace_lines)\n",
    "    elif (len([l for l in updated_stackTrace_lines[1:] if l.endswith(\".invoke0(Native Method)\")])>0):\n",
    "        split_line_by = \"Native_Method\"\n",
    "        lines = [i for i in updated_stackTrace_lines if i.endswith(\".invoke0(Native Method)\")]\n",
    "        stoped_index = updated_stackTrace_lines.index(lines[0])\n",
    "        target_lines = updated_stackTrace_lines[:stoped_index]\n",
    "        return target_lines,split_line_by,len(updated_stackTrace_lines)\n",
    "    elif (len([l for l in updated_stackTrace_lines[1:] if l.endswith(\".invoke(Unknown Source)\")])>0):\n",
    "        split_line_by = \"Unknown_Source\"\n",
    "        lines = [i for i in updated_stackTrace_lines if i.endswith(\".invoke(Unknown Source)\")]\n",
    "        stoped_index = updated_stackTrace_lines.index(lines[0])\n",
    "        target_lines = updated_stackTrace_lines[:stoped_index]\n",
    "        return target_lines,split_line_by,len(updated_stackTrace_lines)\n",
    "    else:\n",
    "        result = []\n",
    "        for idx, val in enumerate(updated_stackTrace_lines):\n",
    "            if (val.startswith(test_name.rsplit(\".\",1)[0])):\n",
    "                result.append(idx)\n",
    "        split_index = 5\n",
    "        if (len(result)>0):\n",
    "            split_index = max(result)\n",
    "        split_line_by = \"top-5-lines\"\n",
    "        target_lines = updated_stackTrace_lines[:split_index+1]\n",
    "        return target_lines,split_line_by,len(updated_stackTrace_lines) \n",
    "\n",
    "def fixStactTraces(lines):\n",
    "    fixed = []\n",
    "    for line in lines:\n",
    "        if (')at' in line):\n",
    "            fixed.append(line.split(')at')[0]+')')\n",
    "            fixed.append(line.split(')at')[1])\n",
    "        else:\n",
    "            fixed.append(line)\n",
    "    return fixed\n",
    "\n",
    "def getResultFromTestBlock(xmlroot):\n",
    "    # keys are: id|freq|project|source|status\n",
    "    # values  : exception|#|StackTrace                    \n",
    "    testId = 0        \n",
    "    testResultPerTestName = {}\n",
    "    for test in xmlroot.iter('test'):\n",
    "        if (type(test.find('test_exception').text) is str and type(test.find('test_stackTrace').text) is str):\n",
    "            testId = testId + 1\n",
    "            key = str(testId)+'|'+str(test.find(\"test_name\").attrib[\"frequency\"])+'|'+str(test.find(\"test_name\").attrib[\"project\"])+'|'+str(test.find(\"test_name\").attrib[\"source\"])+'|'+str(test.find(\"test_name\").attrib[\"status\"])                        \n",
    "            # exceptions part .. \n",
    "            exception_type = get_main_exception_type(test.find('test_exception').text.lstrip().rstrip())\n",
    "            # stackTrace Part \n",
    "            target_lines,split_line_by, total_stacktrace_lines = get_stacktrace(((test.find('test_stackTrace').text).lstrip()).rstrip(),test.find(\"test_name\").text)\n",
    "\n",
    "            # Condition ( to lines in one line ' split by ')at')\n",
    "            target_lines = fixStactTraces(target_lines)\n",
    "            testResultPerTestName[key]=exception_type+\"|#|\"+'|'.join(target_lines)\n",
    "    \n",
    "    return testResultPerTestName\n",
    "\n",
    "def findUniqueFailures(testResultPerTestName,tests_xml, test_name):\n",
    "    uniqueFailures = set([v for k,v in testResultPerTestName.items()])\n",
    "    for failure in uniqueFailures:\n",
    "        testResultByFailure = {k:v for k,v in testResultPerTestName.items() if v == failure}\n",
    "        totalFreq = sum([int(k.split(\"|\")[1]) for k,v in testResultByFailure.items()])\n",
    "        testId = list(testResultByFailure.keys())[0].split(\"|\")[0]\n",
    "        project = list(testResultByFailure.keys())[0].split(\"|\")[2]\n",
    "        source = list(testResultByFailure.keys())[0].split(\"|\")[3]\n",
    "        status = list(testResultByFailure.keys())[0].split(\"|\")[4]\n",
    "        tests_xml = add_xml_block(tests_xml,\"test\",test_name,totalFreq,testId,status,source,failure.split(\"|#|\")[0],failure.split(\"|#|\")[1].split('|'),project)\n",
    "    return tests_xml\n",
    "\n",
    "\n",
    "def getResultFromMutationBlock(xmlroot):\n",
    "    mutantInd = 0\n",
    "    mutantResultPerTestName = {}     \n",
    "    for mutant in xmlroot.iter('mutant'):\n",
    "        mutantInd = mutantInd + 1\n",
    "        key = str(mutantInd)+'|##|'+ mutant.find('mutant_name').attrib['mutant_id'] +'|##|' + mutant.find('mutant_name').attrib['source'] + '|#|' + mutant.find('mutant_name').attrib['status'] +'|#|' + mutant.find('mutant_name').attrib['numberOfKills'] +'|#|' + str(mutant.find('mutant_name').attrib['killedByThisException'])\n",
    "        exception_type = get_main_exception_type(mutant.find('mutant_exception').text.lstrip().rstrip())\n",
    "        target_lines,split_line_by, total_stacktrace_lines = get_stacktrace(((mutant.find('mutant_stackTrace').text).lstrip()).rstrip(),mutant.find(\"mutant_name\").text)        \n",
    "        mutantResultPerTestName[key]=exception_type+\"|#|\"+'|'.join(target_lines)\n",
    "    return mutantResultPerTestName\n",
    "\n",
    "def add_xml_block_for_mutants(mutantPartXML,mutantAttrib,test_name,exception,stackTrace):\n",
    "    mutant = ET.SubElement(mutantPartXML, \"mutant\")\n",
    "    ET.SubElement(mutant, \"mutant_name\", mutantAttrib).text = test_name\n",
    "    mutant_exception = ET.SubElement(mutant, \"mutant_exception\")    \n",
    "    mutant_exception.text = exception\n",
    "        \n",
    "    mutant_stacktracke = ET.SubElement(mutant, \"mutant_stackTrace\")\n",
    "    for line in stackTrace:\n",
    "        ET.SubElement(mutant_stacktracke, \"line\").text = line\n",
    "    \n",
    "    return mutantPartXML\n",
    "\n",
    "def findUniqueMutations(mutationResultPerTestName,mutantPartXML, test_name):\n",
    "    # We ignore any mutation that have two different exceptions and stack trace lines. .. \n",
    "    mutatoinAttrib= ['mutant_id','source','status','numberOfKills','killedByThisException']\n",
    "    mutationIds = [int(i.split('|##|')[1]) for i in mutationResultPerTestName.keys()]\n",
    "    for mutantId in sorted(set(mutationIds)):\n",
    "        mutationTracer = 0\n",
    "        resultPerId = {k:v for k,v in mutationResultPerTestName.items() if '|##|'+str(mutantId) + '|##|' in k}\n",
    "        if (len(resultPerId)==1):\n",
    "            mutationTracer = 1\n",
    "        else:\n",
    "            if (len(set([v for v in resultPerId.values()])) == 1):\n",
    "                mutationTracer = 1\n",
    "        if (mutationTracer>0):\n",
    "            mutationAttribValues = list(resultPerId.keys())[0].split('|##|')[2].split('|#|')\n",
    "            mutationAttribValues.insert(0,str(mutantId))\n",
    "            mutantAttrib = dict(zip(mutatoinAttrib, mutationAttribValues))\n",
    "            exception = list(resultPerId.values())[0].split('|#|')[0]\n",
    "            stackTraceLines = list(resultPerId.values())[0].split('|#|')[1].split('|')\n",
    "            if (stackTraceLines[0] != \"\"):\n",
    "                mutantPartXML = add_xml_block_for_mutants(mutantPartXML,mutantAttrib,test_name,exception,stackTraceLines)\n",
    "    \n",
    "    return mutantPartXML\n",
    "\n",
    "def addPITdetails(summaryXML,total):\n",
    "    summaryXML.findall('./mutants')[0].set('Total', str(total))\n",
    "    summaryXML.findall('./mutants')[0].set('Killed', str([t.find('mutant_name').attrib['status'] for t in summaryXML.findall('.//mutant')].count('KILLED')))\n",
    "    summaryXML.findall('./mutants')[0].set('Flaky', str([t.find('mutant_name').attrib['status'] for t in summaryXML.findall('.//mutant')].count('FLAKY')))\n",
    "    return summaryXML\n",
    "\n",
    "\n",
    "def processMutations(mutationsData,test_name):\n",
    "    removedKeys = []\n",
    "    keys = {k:v for k,v in mutationsData.items() if int(k.split('|#|')[2])>20}\n",
    "    values = [v.split('|#|')[0] for v in keys.values()]\n",
    "    mutantIDsWith40Runs = []\n",
    "    for k1 in keys.keys():\n",
    "        if ('KILLED' in k1):\n",
    "            if (int(k1.split('|#|')[2])==40):\n",
    "                if (k1.split('|##|')[1] not in mutantIDsWith40Runs):\n",
    "                    mutantIDsWith40Runs.append(k1.split('|##|')[1])\n",
    "                else:\n",
    "                    removedKeys.append(k1)\n",
    "            elif (int(k1.split('|#|')[2])==32): # This is the case of ninja exceptions ... \n",
    "                if (int(k1.split('|#|')[3])==12):\n",
    "                    removedKeys.append(k1)\n",
    "    for remove in set(removedKeys):\n",
    "        del mutationsData[remove]\n",
    "    return mutationsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettTestInfo(xmlFile):\n",
    "    testName = xmlFile.findall('.//test')[0].find('test_name').text\n",
    "    className = testName.rsplit('.',1)[0]\n",
    "    projectName = xmlFile.findall('.//test')[0].find('test_name').attrib['project']\n",
    "    return testName,className,projectName\n",
    "\n",
    "def readClassNames(classNamesDir):\n",
    "    allClassNames = pd.DataFrame(columns=['Project','ClassNameType','ClassName'])\n",
    "    for classType in ['Test','CUT']:\n",
    "        allTestsFiles = getFilesByEndsWith(classNamesDir,classType+'.txt')\n",
    "        for file in allTestsFiles:\n",
    "            if (file.endswith('/'+classType+'.txt')):\n",
    "                project = file.split(classNamesDir+'/')[1].split('/')[0]\n",
    "                with open(file) as f:\n",
    "                    classNames = f.readlines()\n",
    "                    classNames_list = [x.strip() for x in classNames]\n",
    "\n",
    "                    for className in classNames_list:\n",
    "                        allClassNames = allClassNames.append(pd.Series([project,classType,className], index=allClassNames.columns ), ignore_index=True)\n",
    "    return allClassNames\n",
    "\n",
    "def analyzeStackTraceLines(test,testName,className,projectName,allClassNames):\n",
    "    testNameVal = 0\n",
    "    classNameVal = 0\n",
    "    otherTestVal = 0\n",
    "    jUnitVal = 0\n",
    "    CUTlines = 0\n",
    "    \n",
    "    projectRepoName = allClassNames.loc[allClassNames['ClassName'] == className, 'Project'].iloc[0]\n",
    "    classNamesPerProject = allClassNames[allClassNames['Project']==projectRepoName]\n",
    "    testClassNames = classNamesPerProject[classNamesPerProject['ClassNameType']=='Test']['ClassName'].unique()\n",
    "    cutClassNames = classNamesPerProject[classNamesPerProject['ClassNameType']=='CUT']['ClassName'].unique()\n",
    "    \n",
    "    for line in test.iter('line'):\n",
    "        if (line.text.startswith(testName)):\n",
    "            testNameVal = 1\n",
    "        else:\n",
    "            if (line.text.startswith(className)):\n",
    "                classNameVal = 1\n",
    "            elif(any(line.text.startswith(x) for x in testClassNames)):\n",
    "                otherTestVal = 1\n",
    "            elif(line.text.startswith('org.junit.') or line.text.startswith('junit.framework')):\n",
    "                jUnitVal = 1\n",
    "            elif(any(line.text.startswith(y) for y in cutClassNames)):\n",
    "                CUTlines = 1\n",
    "                \n",
    "    return testNameVal,classNameVal,otherTestVal,jUnitVal,CUTlines\n",
    "\n",
    "def readTxtFile(file):\n",
    "    with open(file) as f:\n",
    "        classNames = f.readlines()\n",
    "        classNames_list = [x.strip() for x in classNames]\n",
    "        return classNames_list\n",
    "\n",
    "def getAllClassesNames(classesNames,testClassesNames,projectName):\n",
    "    allClassNames = pd.DataFrame(columns=['Project','ClassNameType','ClassName'])\n",
    "    for className in classesNames:\n",
    "        if ('<' in className or '$' in className):\n",
    "            if ('<' in className.rsplit('.',1)[1]):\n",
    "                className = className.rsplit('<',1)[0]\n",
    "            if ('$' in className.rsplit('.',1)[1]):\n",
    "                className = className.rsplit('$',1)[0]\n",
    "        allClassNames = allClassNames.append(pd.Series([projectName,\"CUT\",className], index=allClassNames.columns ), ignore_index=True)\n",
    "        \n",
    "    for testClassName in testClassesNames:\n",
    "        if ('<' in className or '$' in className):\n",
    "            if ('<' in testClassName.rsplit('.',1)[1]):\n",
    "                testClassName = testClassName.rsplit('<',1)[0]\n",
    "            if ('$' in testClassName.rsplit('.',1)[1]):\n",
    "                testClassName = testClassName.rsplit('$',1)[0]\n",
    "\n",
    "        allClassNames = allClassNames.append(pd.Series([projectName,\"Test\",testClassName], index=allClassNames.columns ), ignore_index=True)\n",
    "    return allClassNames\n",
    "\n",
    "def findDuplicateFailures(summaryFiles):\n",
    "    for testFile in tqdm (summaryFiles,desc=\"Completed...\"):\n",
    "        if (os.path.getsize(testFile) >0):\n",
    "            testXMLfile = readXMLFile(testFile)\n",
    "            if (len(testXMLfile.findall('.//test'))>0):\n",
    "                uniqueFailures = {}\n",
    "                for test in testXMLfile.findall('.//mutant'):\n",
    "                    status = test.find('mutant_name').attrib['status']\n",
    "                    mutantId = test.find('mutant_name').attrib['mutant_id']\n",
    "                    testException =  test.find('mutant_exception').text\n",
    "                    stackTraceLines =  [line.text for line in test.iter('line')]\n",
    "                    if (status+'#'+testException+'#'+'|'.join(stackTraceLines) not in uniqueFailures.values()):\n",
    "                        uniqueFailures[int(mutantId)] = status+'#'+testException+'#'+'|'.join(stackTraceLines)\n",
    "                \n",
    "                FeaturesPerTest = pd.read_csv(os.path.join(testFile.rsplit('/',1)[0],'FeaturesPerTest.csv'))\n",
    "                FeaturesPerTest['DuplicateFailure'] = 0\n",
    "                for index, row in FeaturesPerTest.iterrows():\n",
    "                    if row['FailureType'] == 'mutant':\n",
    "                        if (row['FailureId'] not in list(uniqueFailures.keys())):\n",
    "                            FeaturesPerTest.at[index,'DuplicateFailure']=1\n",
    "\n",
    "                FeaturesPerTest.to_csv(os.path.join(testFile.rsplit('/',1)[0],'FeaturesPerTest.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the 22 project dataset\n",
    "datasetDir = \"Path-to-22-projects/ICST-Final-Dataset\"\n",
    "\n",
    "allXmlFiles = getFilesByEndsWith(datasetDir,'.xml')\n",
    "testXmlFiles = [t for t in allXmlFiles if not t.split('/')[-1].startswith('summary-of-') and not t.split('/')[-1].startswith('pit-report')]\n",
    "\n",
    "\n",
    "pitIndexPerTest = 'Path-to/target-index-per-test.json'\n",
    "targetIndex = read_json_file(pitIndexPerTest)\n",
    "\n",
    "\n",
    "#### Pair of test-xml-file : PITs reports\n",
    "allPITsReports = getFilesByStartsWith(datasetDir,'pit-report')\n",
    "\n",
    "pitTests = list(set([t.split('/')[-2] for t in allPITsReports]))\n",
    "dataset = {}\n",
    "for pit in pitTests:\n",
    "    dataset[pit.replace('#','.')] = [p for p in allPITsReports if pit+'/' in p]\n",
    "\n",
    "\n",
    "\n",
    "#### This is for Step 3\n",
    "parser = etree.XMLParser(strip_cdata=False,recover=True)\n",
    "classNamesDir = 'Path-to/FlakeFlagger-testCode-CUT-reports'\n",
    "allClassNames = readClassNames(classNamesDir)\n",
    "summaryFiles = getFilesByStartsWith(datasetDir,'summary-of-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0): which index-per-pit-per-test I need to parse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alreadyParsedXML = read_json_file(pitIndexPerTest)\n",
    "targetMutations = ['KILLED','SURVIVED','FLAKY']\n",
    "\n",
    "overwrite = False # Chenge it if you like to parse the pit reports again. \n",
    "if (overwrite):\n",
    "    alreadyParsedXML = {}\n",
    "    for k in tqdm(list(dataset.keys())):\n",
    "        if (k not in alreadyParsedXML.keys()):\n",
    "            mutantXmlFile = readXMLFile(dataset[k][0])\n",
    "            alreadyParsedXML[k]= ([ind for ind in range(0,len(mutantXmlFile.findall('./mutation'))) if mutantXmlFile.findall('./mutation')[ind].attrib['status'] in targetMutations])\n",
    "            save_dict_to_json(alreadyParsedXML, 'data-input/target-index-per-test.json')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1) Clean test-xml-file before merge mutations. The merge will be on the same xml file as 'Mutants' block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 683/683 [00:00<00:00, 6721.97it/s]\n"
     ]
    }
   ],
   "source": [
    "alreadyParsedXML = read_json_file(pitIndexPerTest)\n",
    "\n",
    "for test in tqdm(list(dataset.keys())):\n",
    "    test_name = [t for t in testXmlFiles if t.endswith(test+'.xml')]\n",
    "    testXML = [t for t in testXmlFiles if t.endswith(test+'.xml')]\n",
    "    if (len(test_name)>0 and test in dataset.keys() and test in alreadyParsedXML.keys()): \n",
    "        # This IF is because not every pit has test-xml-file (not flaky test) or there is no pit reports\n",
    "\n",
    "        # read the xml file \n",
    "        testXML = readXMLFile(test_name[0])\n",
    "\n",
    "        # 1) Prepare the xml file before mering the mutations\n",
    "        testFailuresOnly = cleanTestXML(testXML)\n",
    "\n",
    "        # 2) Temp: remove test_dir if exist (no longer needed)\n",
    "        testFailuresOnly = removeTestDirTag(testFailuresOnly)\n",
    "\n",
    "        # 3) Get killed mutants per test and their exceptions: Dictionary (k = mutantId (index) and V = list of exceptions)\n",
    "        killedMutants = collectKilledMutants(dataset[test],alreadyParsedXML[test])\n",
    "\n",
    "        # 4) Merge mutants to the XML file .. \n",
    "        mergeXML = mergedMutants(testFailuresOnly,killedMutants,test)\n",
    "        \n",
    "        # 5) Save mergeFile \n",
    "        writeXML(testFailuresOnly,test_name[0].rsplit('.xml',1)[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2) Summarize each test-xml-file to consider exception type and stacktraces only (as designed by our paper ICST2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 683/683 [02:26<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for test_name in tqdm(list(dataset.keys())):\n",
    "    testXML = [t for t in testXmlFiles if t.endswith(test_name+'.xml')]\n",
    "    if (len(testXML)>0):\n",
    "        if (os.path.getsize(testXML[0])>0):\n",
    "            # 1) read the xml file\n",
    "            xmlroot = readXMLFile(testXML[0])\n",
    "\n",
    "            # 2) Create plain xml file for summary-of- file\n",
    "            summaryXML, testPartXML, mutantPartXML = createSummaryXMLPlainFile()\n",
    "            \n",
    "            # 3) get the data from each test block first \n",
    "            testResultPerTestName = getResultFromTestBlock(xmlroot)\n",
    "\n",
    "            # 4) parse the test result first .. \n",
    "            testPartXML = findUniqueFailures(testResultPerTestName, testPartXML, test_name)\n",
    "\n",
    "            # 5) get the data from each mutation block\n",
    "            mutationResultPerTestNameOriginal = getResultFromMutationBlock(xmlroot)\n",
    "\n",
    "            # 5a) process the collected mutants  \n",
    "            mutationResultPerTestName = processMutations(mutationResultPerTestNameOriginal,test_name)       \n",
    "\n",
    "            # save_dict_to_json(mutationResultPerTestName,'file_name.json')\n",
    "            # stillExtra = [k.split('|##|')[1] for k in mutationResultPerTestName.keys() if int(k.split('|#|')[2])>20 and 'KILLED' in k]\n",
    "            # for s in set(stillExtra):\n",
    "            #     c = [k.split('|#|')[-1] for k in mutationResultPerTestName.keys() if k.split('|##|')[1] == s]\n",
    "            #     if (sum([int(ss) for ss in c])>20):\n",
    "            #         test20X.append(test_name)\n",
    "\n",
    "            # 6) parse the mutantions result for each test .. \n",
    "            mutantPartXML = findUniqueMutations(mutationResultPerTestName,mutantPartXML, test_name)\n",
    "\n",
    "            # 7) EXTRA: add pit details (how many mutants/ how many are killed)\n",
    "            summaryXML = addPITdetails(summaryXML,len(readXMLFile(dataset[test_name][0]).findall('./mutation')))\n",
    "            \n",
    "            # # 8) export the summary xml file \n",
    "            outputFile = testXML[0].rsplit('/',1)[0]+'/summary-of-'+ testXML[0].rsplit('/',1)[1]\n",
    "            abstract_tree = ET.ElementTree(summaryXML)\n",
    "            abstract_tree.write(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3) Collect the features (Table 1 in the paper) from each failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for testFile in tqdm(summaryFiles):\n",
    "    if (os.path.getsize(testFile) >0):\n",
    "        testXMLfile = readXMLFile(testFile)\n",
    "        if (len(testXMLfile.findall('.//test'))>0):\n",
    "            perTest = []\n",
    "            testName,className,projectName = gettTestInfo(testXMLfile)\n",
    "            \n",
    "            if (className in allClassNames['ClassName'].unique()):\n",
    "                for failure in ['test','mutant']:\n",
    "                    for test in testXMLfile.findall('.//'+failure):\n",
    "                        status = test.find(failure+'_name').attrib['status']\n",
    "                        testException =  test.find(failure+'_exception').text\n",
    "                        if (failure == 'test'):\n",
    "                            rowId = test.find(failure+'_name').attrib['id']\n",
    "                        else:\n",
    "                            rowId = test.find(failure+'_name').attrib['mutant_id']\n",
    "                        \n",
    "                        testNameExist,classNameExist,otherTestNameExist,junitLinesExist,cutLinesExist = analyzeStackTraceLines(test,testName,className,projectName,allClassNames)\n",
    "                        \n",
    "\n",
    "                        # consider all flaky failures .. \n",
    "                        maxFreq = 1\n",
    "                        if (failure == \"test\"):\n",
    "                            maxFreq = int(test.find(failure+'_name').attrib['frequency'])\n",
    "                        for i in range (0,maxFreq):\n",
    "                            perFailure = []\n",
    "                            perFailure.append(projectName)\n",
    "                            perFailure.append(testName)\n",
    "                            perFailure.append(failure)\n",
    "                            perFailure.append(rowId)\n",
    "                            perFailure.append(status)\n",
    "                            perFailure.append(testException)\n",
    "                            perFailure.append(testNameExist)\n",
    "                            perFailure.append(classNameExist)\n",
    "                            perFailure.append(otherTestNameExist)\n",
    "                            perFailure.append(junitLinesExist)\n",
    "                            perFailure.append(cutLinesExist)\n",
    "                            perTest.append(perFailure)\n",
    "\n",
    "                perTestdf = pd.DataFrame(perTest, columns =['Project','Test','FailureType','FailureId','FailureStatus','FailureException','TestNameInStackTrace','ClassNameInStackTrace','otherTestClassInStackTrace','JunitInStackTrace','CUTinStackTrace'])\n",
    "\n",
    "                # This step to add label if a test failure has mutants or not\n",
    "                if (len(testXMLfile.findall('.//mutant'))>0):\n",
    "                    perTestdf['HasMutants'] = 1\n",
    "                else:\n",
    "                    perTestdf['HasMutants'] = 0\n",
    "                    \n",
    "                perTestdf.to_csv(testFile.rsplit('/',1)[0]+'/FeaturesPerTest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
