{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Failure Log Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import operator\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import filecmp\n",
    "from lxml import etree\n",
    "import subprocess\n",
    "import os\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score,recall_score, confusion_matrix,precision_recall_fscore_support\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import export_text\n",
    "import math\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tqdm import tqdm\n",
    "from sklearn import tree\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesByEndsWith(fileDir,ends):\n",
    "    filesList = []\n",
    "    for path, subdirs, files in os.walk(fileDir):\n",
    "        for file in files:\n",
    "            if (file.endswith(ends)):\n",
    "                filesList.append(os.path.join(path,file))\n",
    "    return filesList\n",
    "\n",
    "def getFilesByStartsWith(fileDir,starts):\n",
    "    filesList = []\n",
    "    for path, subdirs, files in os.walk(fileDir):\n",
    "        for file in files:\n",
    "            if (file.startswith(starts)):\n",
    "                filesList.append(os.path.join(path,file))\n",
    "    return filesList\n",
    "\n",
    "def readXMLFile(filePath):\n",
    "    parser = etree.XMLParser(strip_cdata=False,recover=True)\n",
    "    with open(filePath, \"rb\") as source:\n",
    "        tree = etree.parse(source, parser=parser)\n",
    "    xmlroot = tree.getroot()\n",
    "    return xmlroot\n",
    "\n",
    "\n",
    "def getTestNames(allTestNames):\n",
    "    testDict = {}\n",
    "    i = 1000\n",
    "    for e in allTestNames:\n",
    "        testDict[e] = i\n",
    "        i = i + 1\n",
    "    return testDict\n",
    "\n",
    "def get_scores (tn,fp,fn,tp):\n",
    "    if(tp==0):\n",
    "        accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "        Precision = 0\n",
    "        Recall = 0\n",
    "        F1 = 0    \n",
    "    else:\n",
    "        accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "        Precision = tp/(tp+fp)\n",
    "        Recall = tp/(tp+fn)\n",
    "        F1 = 2*((Precision*Recall)/(Precision+Recall))    \n",
    "    return accuracy, F1, Precision, Recall\n",
    "\n",
    "def get_rates(tn,fp,fn,tp):\n",
    "    tpr = tp/(tp+fn)\n",
    "    fpr = fp/(tn+fp)\n",
    "    return tpr, fpr\n",
    "\n",
    "\n",
    "def getResultTag(failuresPerProject,uniqueFailureFilter,onlyFailureWithMutantsFilter,balance):\n",
    "    resultTag = ''\n",
    "    if (balance):\n",
    "        resultTag = resultTag+\"Balance\"\n",
    "    else:\n",
    "        resultTag = resultTag+\"NotBalance\"\n",
    "\n",
    "    if (uniqueFailureFilter):\n",
    "        resultTag = resultTag +'+NoDuplicate'\n",
    "    else:\n",
    "        resultTag = resultTag +'+Duplicate'\n",
    "        \n",
    "    if (onlyFailureWithMutantsFilter):\n",
    "        failuresPerProject = failuresPerProject[failuresPerProject[\"HasMutants\"]>0]\n",
    "        resultTag = resultTag +'+FailuresWithMutants'\n",
    "    else:\n",
    "        resultTag = resultTag +'+allFailures'\n",
    "    \n",
    "    return resultTag,failuresPerProject\n",
    "    \n",
    "\n",
    "def oneHotEncodingFeatures(dataSet,encoderColumn):\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    dataSetEncoded = dataSet[[encoderColumn]]\n",
    "    ohe.fit(dataSetEncoded)\n",
    "\n",
    "    dataSetEncoded_ohe = ohe.transform(dataSetEncoded).toarray()\n",
    "    encoded_df = pd.DataFrame(dataSetEncoded_ohe, columns=ohe.get_feature_names(dataSetEncoded.columns))\n",
    "\n",
    "    concatResult = pd.concat([encoded_df,dataSet.loc[:, dataSet.columns != encoderColumn]], axis=1)\n",
    "    return concatResult\n",
    "\n",
    "def getConfusionMatrixPerRow(dataset,confusionMatrixTracker,project,resultTag,dfColumnet):    \n",
    "    ConfResult = []\n",
    "    for resultType in set(confusionMatrixTracker.values()):\n",
    "        indPerType = [k for k,v in confusionMatrixTracker.items() if v == resultType]\n",
    "        perResult = dataset.iloc[indPerType]\n",
    "        ExceptionsList = perResult['FailureException'].values.tolist()\n",
    "        for exception in set(ExceptionsList):\n",
    "            ConfResult.append([project,resultTag,resultType,exception,ExceptionsList.count(exception),(ExceptionsList.count(exception)/len(ExceptionsList))*100])\n",
    "        \n",
    "    ConfResultDF = pd.DataFrame(ConfResult,columns=dfColumnet)\n",
    "    return ConfResultDF\n",
    "\n",
    "def PredictFailure(data,balance,targetCol,classifier):\n",
    "    data_target = data[[targetCol]]\n",
    "    data = data.drop([targetCol], axis=1)\n",
    "    if ('index' in data.columns):\n",
    "        data = data.drop(['index'], axis=1)\n",
    "\n",
    "    # The type of k-fold is StratifiedKFold to ensure each fold has flaky failures. \n",
    "    fold = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "\n",
    "    TN = FP = FN = TP = 0\n",
    "    confusionMatrixTracker = {}\n",
    "\n",
    "    for train_index, test_index in fold.split(data,data_target):\n",
    "        x_train, x_test = data.iloc[list(train_index)], data.iloc[list(test_index)]\n",
    "        y_train, y_test = data_target.iloc[list(train_index)], data_target.iloc[list(test_index)]\n",
    "\n",
    "        if(balance):\n",
    "            oversample = SMOTE()\n",
    "            x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
    "        if (classifier==\"DT\"):\n",
    "            model = DecisionTreeClassifier(criterion='entropy', max_depth = None)\n",
    "        elif(classifier == \"NB\"):\n",
    "            model = GaussianNB()\n",
    "        trained_model = model.fit(x_train, y_train)\n",
    "        preds = trained_model.predict(x_test)\n",
    "\n",
    "        # Export the DT as text\n",
    "        #print (tree.export_text(model,feature_names=data[1,2,3,4,5,6]))\n",
    "        \n",
    "        # Track Which one is FP, FN, TP ( others are TN)\n",
    "        y_testList = y_test[targetCol].values.tolist()\n",
    "        predsList = preds.tolist()\n",
    "        for counter in range (0,len(test_index)):\n",
    "            if (predsList[counter] == 1 & y_testList[counter]==1):\n",
    "                confusionMatrixTracker[test_index[counter]] = 'TP'\n",
    "            elif ((predsList[counter] == 1) & (y_testList[counter]==0)):\n",
    "                confusionMatrixTracker[test_index[counter]] = 'FP'\n",
    "            elif ((predsList[counter] == 0) & (y_testList[counter]==1)):\n",
    "                confusionMatrixTracker[test_index[counter]] = 'FN'\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds, labels=[0,1]).ravel()\n",
    "        TN = TN + tn\n",
    "        FP = FP + fp\n",
    "        FN = FN + fn\n",
    "        TP = TP + tp\n",
    "    accuracy, F1, Precision, Recall = get_scores (TN,FP,FN,TP)\n",
    "    tpr, fpr = get_rates(TN,FP,FN,TP)\n",
    "    return [TN+FP+FN+TP,TP,FN,FP,TN,Precision*100, Recall*100,F1*100,accuracy*100,tpr*100, fpr*100],confusionMatrixTracker\n",
    "\n",
    "\n",
    "def getClassificationResult(failuresDataset,datasetColumns,targetCol,classifier):\n",
    "    resultColumns = ['ResultTag','Project','Total','TP','FN','FP','TN','P','R','F1','Ac','TPR','FPR']\n",
    "    binaryResult = [True,False] # To consider the repetition of failures from different mutants + Consider the fact that some tests have no mutants + to balance or not balance.\n",
    "    resultDF = pd.DataFrame(columns=resultColumns)\n",
    "\n",
    "    # Final target tags\n",
    "    targetTags = [\"NotBalance+Duplicate+FailuresWithMutants\",\"Balance+Duplicate+FailuresWithMutants\"]\n",
    "\n",
    "    # for confusion matrix analysis \n",
    "    defColumns = ['ResultTag','Project','PredictResultTag','FailureException','FailureExceptionFreq','FailureExceptionPercentage']\n",
    "    confusionDF = pd.DataFrame(columns=defColumns)\n",
    "    for uniqueFailureFilter in binaryResult:\n",
    "        for onlyFailureWithMutantsFilter in binaryResult:\n",
    "            for balance in binaryResult:\n",
    "                for project in failuresDataset['Project'].unique():\n",
    "                    failuresPerProject = failuresDataset[failuresDataset['Project']==project]\n",
    "                    # Result tag:\n",
    "                    resultTag,failuresPerProject = getResultTag(failuresPerProject,uniqueFailureFilter,onlyFailureWithMutantsFilter,balance)\n",
    "                    \n",
    "                    # # temp step :\n",
    "                    if (resultTag in targetTags):\n",
    "                        # this to ensure that we have enought flaky failures + have at least one mutant\n",
    "                        if (failuresPerProject[targetCol].sum()>10 and 'mutant' in failuresPerProject['FailureType'].unique()):\n",
    "                            trainingFailures = failuresPerProject[datasetColumns].reset_index()\n",
    "                            # OneHotEncode ... for FailureException or Test\n",
    "                            if ('FailureException' in datasetColumns):\n",
    "                                trainingFailures = oneHotEncodingFeatures(trainingFailures,'FailureException')\n",
    "                            if ('Test' in datasetColumns):\n",
    "                                trainingFailures = oneHotEncodingFeatures(trainingFailures,'Test')\n",
    "\n",
    "                            # Predict ...  \n",
    "                            predictionResult,confusionMatrixTracker = PredictFailure(trainingFailures,balance,targetCol,classifier)\n",
    "                            predictionResult.insert(0,project)\n",
    "                            predictionResult.insert(0,resultTag)\n",
    "\n",
    "                            # Get the distribution of confusion matrix per result\n",
    "                            confusionMatrixTrackerPerProject = getConfusionMatrixPerRow(failuresPerProject,confusionMatrixTracker,project,resultTag,defColumns)\n",
    "                            confusionDF = confusionDF.append(confusionMatrixTrackerPerProject)\n",
    "\n",
    "                            resultDF = resultDF.append(pd.Series(predictionResult, index=resultDF.columns ), ignore_index=True)\n",
    "                        else:\n",
    "                            resultDF = resultDF.append(pd.Series([resultTag,project,0,0,0,0,0,0,0,0,0,0,0], index=resultDF.columns ), ignore_index=True)\n",
    "    return resultDF, confusionDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'Result'\n",
    "xmlSummaryDir = 'Path-to-summary-files' # add the path to the 22 java projects (ICST dataset)\n",
    "parser = etree.XMLParser(strip_cdata=False,recover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFiles = getFilesByEndsWith(xmlSummaryDir,'FeaturesPerTest.csv') # use FeaturesPerTestAll.csv to count all failures .. \n",
    "failureFeatures = pd.concat([pd.read_csv(f,index_col=False) for f in csvFiles])\n",
    "\n",
    "# Temp Part for cleaning the columns:\n",
    "UnnamedColumns = [col for col in failureFeatures.columns if col.startswith('Unnamed')]\n",
    "failureFeatures = failureFeatures.drop(columns=UnnamedColumns)\n",
    "\n",
    "if ('CUTnStackTrace' in failureFeatures.columns):\n",
    "    failureFeatures = failureFeatures.drop(columns=['CUTnStackTrace'])\n",
    "\n",
    "# normalized failure Status + FailureType\n",
    "failureFeatures['FailureStatusCode'] = [1 if x =='FLAKY' else 0 for x in failureFeatures['FailureStatus']]\n",
    "failureFeatures['FailureTypeCode'] = [1 if x =='test' else 0 for x in failureFeatures['FailureType']]\n",
    "\n",
    "\n",
    "# Remove flaky mutants .. \n",
    "execludeFlakyMutant = True\n",
    "if (execludeFlakyMutant):\n",
    "    failureFeatures = failureFeatures[~((failureFeatures['FailureType']=='mutant')&(failureFeatures['FailureStatus']=='FLAKY'))]\n",
    "\n",
    "ignoredTests =[]\n",
    "for t in failureFeatures['Test'].unique():\n",
    "    perTest = failureFeatures[failureFeatures['Test']==t]\n",
    "    if (len(perTest['FailureType'].unique())==1):\n",
    "        ignoredTests.append(t)\n",
    "failureFeaturesAll = failureFeatures[~failureFeatures['Test'].isin(ignoredTests)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Result/PerFailureResult.csv')\n",
    "failureFeaturesFlaky = failureFeaturesAll[failureFeaturesAll['FailureType']=='test']\n",
    "failureFeaturesMutants = failureFeaturesAll[failureFeaturesAll['FailureType']=='mutant']\n",
    "\n",
    "\n",
    "data['TargetFailures'] =  data['Test'] + '|' + data['TestID'].astype(str)\n",
    "failureFeaturesFlaky['TargetFailures'] = failureFeaturesFlaky['Test'] + '|' + failureFeaturesFlaky['FailureId'].astype(str)\n",
    "\n",
    "TargetFailureFeatures = failureFeaturesFlaky[failureFeaturesFlaky['TargetFailures'].isin(data['TargetFailures'].unique())]\n",
    "\n",
    "failureFeatures = TargetFailureFeatures.append(failureFeaturesMutants, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingColumns = ['FailureStatusCode','FailureException', 'TestNameInStackTrace', 'ClassNameInStackTrace','otherTestClassInStackTrace', 'JunitInStackTrace', 'CUTinStackTrace']\n",
    "FlakeFlaggerPredict,FlakeFlaggerConfusionDF = getClassificationResult(failureFeatures,TrainingColumns,'FailureStatusCode','DT')\n",
    "FlakeFlaggerPredictNB,FlakeFlaggerConfusionDFNB = getClassificationResult(failureFeatures,TrainingColumns,'FailureStatusCode','NB')\n",
    "\n",
    "\n",
    "# Label this result .. \n",
    "FlakeFlaggerPredict.insert (0, \"Dataset\", \"FlakeFlagger\")\n",
    "FlakeFlaggerPredict.insert (1, \"ResultType\", \"mainPrediction\")\n",
    "FlakeFlaggerConfusionDF.insert (0, \"Dataset\", \"FlakeFlagger\")\n",
    "FlakeFlaggerConfusionDF.insert (1, \"ResultType\", \"mainPrediction\")\n",
    "\n",
    "FlakeFlaggerPredictNB.insert (0, \"Dataset\", \"FlakeFlagger\")\n",
    "FlakeFlaggerPredictNB.insert (1, \"ResultType\", \"mainPredictionNB\")\n",
    "\n",
    "FlakeFlaggerPredict[FlakeFlaggerPredict['ResultTag']==\"Balance+Duplicate+FailuresWithMutants\"].sort_values(by=['Total'], ascending=False)\n",
    "FlakeFlaggerPredict[FlakeFlaggerPredict['ResultTag']==\"NotBalance+Duplicate+FailuresWithMutants\"].sort_values(by=['Total'], ascending=False)\n",
    "\n",
    "\n",
    "FlakeFlaggerPredict.to_csv(output+'/DT_FinalClassifierResult.csv',index=False)\n",
    "FlakeFlaggerPredictNB.to_csv(output+'/NB_FinalClassifierResult.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
